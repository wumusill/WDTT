{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 케라스로 RNN 구현하기\n",
    "[SimpleRNN 공식문서](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "model.add(SimpleRNN(hidden_units))\n",
    "---------------------------------------------------------------------------\n",
    "# 추가 인자를 사용할 때\n",
    "model.add(SimpleRNN(hidden_units, input_shape=(timesteps, input_dim)))\n",
    "\n",
    "# 다른 표기\n",
    "model.add(SimpleRNN(hidden_units, input_length=M, input_dim=N))\n",
    "```\n",
    "\n",
    "* hidden_units\n",
    "  * dimensionality of the output space, 출력 공간의 차원\n",
    "  * 은닉 상태의 크기를 정의\n",
    "  * 메모리 셀이 다음 시점의 메모리 셀과 출력층으로 보내는 값의 크기(output_dim)와 동일\n",
    "  * RNN의 용량(capacity)을 늘린다고 보면 됨\n",
    "  * 보통 128, 256, 512, 1024 등의 값을 가짐\n",
    "* timesteps\n",
    "  * 입력 시퀀스의 길이(input_length)\n",
    "  * 시점의 수\n",
    "* input_dim\n",
    "  * 입력의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 3)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 18:46:46.630532: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-02-21 18:46:46.630773: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=3, input_shape=(2, 10)))\n",
    "# model.add(SimpleRNN(3, input_length=2, input_dim=10))와 같은 코드\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RNN 층은 (batch_size, timesteps, input_dim) 크기의 3D 텐서를 입력 받음\n",
    "* 자연어에서는 보통\n",
    "  * input_dim : 인코딩 단어 벡터\n",
    "  * timesteps : 단어의 수, 문장의 길이\n",
    "  * batch_size : 문장의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_2 (SimpleRNN)    (8, 3)                    42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, batch_input_shape=(8,2,10)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_1 (SimpleRNN)    (8, 2, 3)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# return_sequences=True\n",
    "# Whether to return the last output in the output sequence, or the full sequence. Default: False.\n",
    "# 출력 값으로 (batch_size, timesteps, output_dim) 크기의 3D Tensor return\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, batch_input_shape=(8,2,10), return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python으로 RNN 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.93679362 0.28312536 0.26017233 0.56932907]\n",
      " [0.44912398 0.80783303 0.04227668 0.82879041]\n",
      " [0.59945913 0.52241481 0.58891286 0.33280499]\n",
      " [0.77877785 0.24336579 0.99086586 0.1379726 ]\n",
      " [0.54610933 0.84598669 0.85937943 0.84233939]\n",
      " [0.41027956 0.98810542 0.2214896  0.80721748]\n",
      " [0.50277082 0.11929504 0.71400028 0.78613042]\n",
      " [0.36680646 0.85468146 0.8977861  0.97187282]\n",
      " [0.61733787 0.45114618 0.66857906 0.46742497]\n",
      " [0.54387829 0.99271519 0.69112894 0.47563419]]\n",
      "초기 은닉 상태 : [0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 문장의 길이\n",
    "timesteps = 10\n",
    "\n",
    "# 입력의 차원, 단어 벡터의 차원\n",
    "input_dim = 4\n",
    "\n",
    "# 은닉 상태의 크기, 메모리 셀의 용량\n",
    "hidden_units = 8\n",
    "\n",
    "inputs = np.random.random((timesteps, input_dim))\n",
    "print(inputs)\n",
    "\n",
    "# 8의 차원을 가지는 벡터, h_{t-1}\n",
    "hidden_state_t = np.zeros((hidden_units))\n",
    "print(\"초기 은닉 상태 :\", hidden_state_t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치와 편향을 각 크기에 맞게 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가중차 Wx의 크기(shape) : (8, 4)\n",
      "가중차 Wh의 크기(shape) : (8, 8)\n",
      "편향 b의 크기(shape) : (8,)\n"
     ]
    }
   ],
   "source": [
    "# 입력에 대한 가중치, 단어 차원에 맞게 설정\n",
    "Wx = np.random.random((hidden_units, input_dim))\n",
    "\n",
    "# 은닉 상태에 대한 가중치\n",
    "Wh = np.random.random((hidden_units, hidden_units))\n",
    "\n",
    "# 편향\n",
    "b = np.random.random((hidden_units))\n",
    "\n",
    "print(\"가중차 Wx의 크기(shape) :\", np.shape(Wx))\n",
    "print(\"가중차 Wh의 크기(shape) :\", np.shape(Wh))\n",
    "print(\"편향 b의 크기(shape) :\", np.shape(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 함수로 예측한 결과 : [0.99998491 0.9998602  0.99999718 0.99998758 0.99843477 0.9999498\n",
      " 0.99997012 0.99998906]\n",
      "가설 함수로 예측한 결과 : [0.99999411 0.99988677 0.99999855 0.99998583 0.99827419 0.99994577\n",
      " 0.99997031 0.99998988]\n",
      "가설 함수로 예측한 결과 : [0.99998381 0.99989015 0.9999971  0.99998765 0.99886745 0.99994869\n",
      " 0.99996718 0.99998153]\n",
      "가설 함수로 예측한 결과 : [0.99997325 0.99989941 0.99999625 0.99999015 0.99917252 0.99995628\n",
      " 0.99997307 0.99997973]\n",
      "가설 함수로 예측한 결과 : [0.99999613 0.9999522  0.99999949 0.9999939  0.99934314 0.99998268\n",
      " 0.99998948 0.99999541]\n",
      "가설 함수로 예측한 결과 : [0.9999954  0.99991695 0.99999907 0.99998861 0.99872396 0.99996442\n",
      " 0.99997596 0.99999191]\n",
      "가설 함수로 예측한 결과 : [0.99998886 0.99988512 0.99999726 0.99998878 0.99866778 0.99991844\n",
      " 0.99998263 0.99998587]\n",
      "가설 함수로 예측한 결과 : [0.99999693 0.99995385 0.99999953 0.99999375 0.99931852 0.99997881\n",
      " 0.9999908  0.9999952 ]\n",
      "가설 함수로 예측한 결과 : [0.99998671 0.99990015 0.99999765 0.99998918 0.9989323  0.99995273\n",
      " 0.99997525 0.99998558]\n",
      "가설 함수로 예측한 결과 : [0.99999321 0.9999396  0.99999912 0.99999174 0.99926495 0.99997957\n",
      " 0.99997894 0.99999153]\n"
     ]
    }
   ],
   "source": [
    "total_hidden_states = []\n",
    "\n",
    "# 문장의 단어들을 하나씩 순회\n",
    "for input_t in inputs:\n",
    "    \n",
    "    # 가설 함수 계산\n",
    "    # Wx * Xt + Wh * Ht-1 + b\n",
    "    output_t = np.tanh(np.dot(Wx, input_t) +  np.dot(Wh, hidden_state_t) + b)\n",
    "    print(\"가설 함수로 예측한 결과 :\", output_t)\n",
    "    total_hidden_states.append(list(output_t))\n",
    "    hidden_state_t = output_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 시점의 은닉 상태 :\n",
      "[[0.99998491 0.9998602  0.99999718 0.99998758 0.99843477 0.9999498\n",
      "  0.99997012 0.99998906]\n",
      " [0.99999411 0.99988677 0.99999855 0.99998583 0.99827419 0.99994577\n",
      "  0.99997031 0.99998988]\n",
      " [0.99998381 0.99989015 0.9999971  0.99998765 0.99886745 0.99994869\n",
      "  0.99996718 0.99998153]\n",
      " [0.99997325 0.99989941 0.99999625 0.99999015 0.99917252 0.99995628\n",
      "  0.99997307 0.99997973]\n",
      " [0.99999613 0.9999522  0.99999949 0.9999939  0.99934314 0.99998268\n",
      "  0.99998948 0.99999541]\n",
      " [0.9999954  0.99991695 0.99999907 0.99998861 0.99872396 0.99996442\n",
      "  0.99997596 0.99999191]\n",
      " [0.99998886 0.99988512 0.99999726 0.99998878 0.99866778 0.99991844\n",
      "  0.99998263 0.99998587]\n",
      " [0.99999693 0.99995385 0.99999953 0.99999375 0.99931852 0.99997881\n",
      "  0.9999908  0.9999952 ]\n",
      " [0.99998671 0.99990015 0.99999765 0.99998918 0.9989323  0.99995273\n",
      "  0.99997525 0.99998558]\n",
      " [0.99999321 0.9999396  0.99999912 0.99999174 0.99926495 0.99997957\n",
      "  0.99997894 0.99999153]]\n"
     ]
    }
   ],
   "source": [
    "total_hidden_states = np.stack(total_hidden_states, axis=0)\n",
    "print('모든 시점의 은닉 상태 :')\n",
    "print(total_hidden_states)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 깊은 순환 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_2 (SimpleRNN)    (None, 10, 8)             112       \n",
      "                                                                 \n",
      " simple_rnn_3 (SimpleRNN)    (None, 10, 8)             136       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 248\n",
      "Trainable params: 248\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 은닉층을 2개 추가하는 경우\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(hidden_units, input_length=10, input_dim=5, return_sequences=True))\n",
    "model.add(SimpleRNN(hidden_units, return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 양방향 순환 신경망, Bidirectional Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 10, 16)           224       \n",
      " l)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 224\n",
      "Trainable params: 224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "timesteps = 10\n",
    "input_dim = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True), input_shape=(timesteps, input_dim)))\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 깊은 양방향 순환 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_1 (Bidirectio  (None, 10, 16)           224       \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 10, 16)           400       \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 10, 16)           400       \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 10, 16)           400       \n",
      " nal)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,424\n",
      "Trainable params: 1,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "timesteps = 10\n",
    "input_dim = 5\n",
    "\n",
    "# 다른 인공 신경망 모델들도 마찬가지이지만, 은닉층을 무조건 추가한다고 해서 모델의 성능이 좋아지는 것은 아님\n",
    "# 은닉층을 추가하면 학습할 수 있는 양이 많아지지만 반대로 훈련 데이터 또한 많은 양이 필요 \n",
    "# 아래의 코드는 은닉층이 4개인 경우\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True), input_shape=(timesteps, input_dim)))\n",
    "model.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True)))\n",
    "model.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True)))\n",
    "model.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True)))\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 케라스 GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 8)                 360       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 360\n",
      "Trainable params: 360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(hidden_units, input_shape=(timesteps, input_dim)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-cert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b37592e24976bbc2b4793f9e16cde29a149096caf81a3ecbdead9daa6a7261c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

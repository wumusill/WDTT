## 🏷️ 워드 임베딩(Word Embedding)

* 각 단어를 벡터화하는 것
* 단어를 `밀집 표현`으로 변환
* 텍스트를 컴퓨터가 이해하고 처리하기 위해 텍스트를 숫자로 변환해야 함
* 단어를 표현하는 방법에 따라 성능에 큰 차이
* 오늘날 가장 많이 사용되는 방법

<br>

### 희소 표현, Sparse Representation
* 원-핫 벡터처럼 값의 대부분을 0으로 표현하는 방법
* 원-핫 벡터는 희소 벡터, `sparse vector`

<br>

* 희소 벡터의 문제점
  * 단어의 개수와 벡터의 차원이 비례
  * 단어의 개수가 늘어나면 벡터의 차원이 한없이 커짐
  * 단어의 index에 해당하는 부분만 1이고 나머지는 0
  * 공간적 낭비
  * 단어의 의미를 표현하지 못함


<br>

### 밀집 표현, Dense Representation
* 희소 표현과 반대되는 표현
* 벡터의 차원을 단어 집합의 크기로 상정하지 않음
* 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤
  * 사용자가 128로 차원을 설정했다면
  * 모든 단어의 벡터 차원은 128로 바뀌면서 모든 값이 실수
* 0, 1 말고도 실수값을 가지게 됨
* 벡터의 차원이 조밀해졌다고 하여 밀집 벡터, `dense vector`라고 함
* 단어 벡터 간 유사도를 계산하여 단어 의미 표현


<br>

### 워드 임베딩, Word Embedding
* 단어를 밀집 벡터(dense vector)의 형태로 바꾸는 것
* 워드 임베딩 과정을 통해 나온 결과를 임베딩 벡터, `embedding vector`라고 함


<br>

* 워드 임베딩 방법론으로는 LSA, Word2Vec, FastText, Glove 등 존재
* 케라스에서는 위 방법을 사용하지 않음
  * 단어를 랜덤한 값을 가지는 밀집 벡터로 변환
  * 인공 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습

<br>

|-|원-핫 벡터|임베딩 벡터|
|:---:|:---:|:---:|
|차원|고차원(단어 집합의 크기)|저차원|
|다른 표현|회소 벡터의 일종|밀집 벡터의 일종|
|표현 방법|수동|훈련 데이터로부터 학습|
|값의 타입|1과 0|실수|

<br>


## 🏷️ 워드투벡터(Word2Vec)
* 단어 벡터 간 유의미한 유사도를 반영할 수 있도록 단어의 의미를 수치화 하는 방법
* 원-핫 벡터는 단어 벡터 간 유사도를 계산할 수 없음
* Word2Vec은 은닉층이 다수인 딥 러닝(deep learning) 모델이 아니라 은닉층이 1개인 얕은 신경망(shallow neural network)
* Word2Vec의 은닉층은 일반적인 은닉층과는 달리 활성화 함수가 존재하지 않음
* 룩업 테이블이라는 연산을 담당하는 층으로 투사층(projection layer)이라고 부르기도 함



<br>

### 희소 표현, Sparse Representation
* 벡터 or 행렬의 값이 대부분 0으로 표현되는 방법
* 이러한 표현 방법은 각 단어 벡터 간 유의미한 유사성을 표현할 수 없음
* 대안으로 단어의 의미를 다차원 공간에 벡터화하는 방법 ➡️ `분산 표현, Distributed Representation`
* 분산 표현을 이용하여 단어 간 의미적 유사성을 벡터화하는 작업 ➡️ `Word Embedding`
* Word Embedding을 통해 표현된 벡터 ➡️ `Embedding Vector`

<br>

### 분산 표현, Distributed Representation
* 분포 가설이라는 가정 하에 만들어진 표현 방법 ➡️ 비슷한 문백에서 등장하는 단어들은 비슷한 의미를 가진다
* 원-핫 벡터처럼 벡터의 차원이 단어 집합의 크기일 필요가 없음 ➡️ 상대적으로 저차원
* 단어 벡터 간 유사도를 계산할 수 있음 ➡️ 대표적인 학습 방법 : `Word2Vec`

<br>

### CBOW(Continuous Bag of Words), Skip-gram
* Word2Vec의 학습 방식
  * `CBOW`
    * 주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하는 방법
  * `Skip-gram`
    * 중간에 있는 단어들을 입력으로 주변에 있는 단어들을 예측하는 방법
  * `Skip-gram`이 `CBOW`보다 성능이 좋다고 알려짐

